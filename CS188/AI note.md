![[Pasted image 20251013151331.png]]
# State Spaces Uninformed Search
A search problem consists of the following elements:

- AÂ **state space**Â - The set of all possible states that are possible in your given world. ä¸åŒåœºæ™¯åŒ…æ‹¬ä¸åŒä¿¡æ¯ï¼š![[Pasted image 20251011172153.png]]
state space sizeè®¡ç®—ï¼š![[Pasted image 20251011172226.png]]
- AÂ **set of actions**Â available in each state
- AÂ **transition model**Â - Outputs the next state when a specific action is taken at current state
- AnÂ **action cost**Â - Incurred when moving from one state to another after applying an action
- AÂ **start state**Â - The state in which an agent exists initially
- AÂ **goal test**Â - A function that takes a state as input, and determines whether it is a goal state

## Search algorithm propertiesï¼š
ï‚§ Complete: Guaranteed to find a solution if one exists? 
ï‚§ Optimal: Guaranteed to find the least cost path?
## State Space Graphs and Search Trees
![[Pasted image 20250919111832.png]]
å¯¹äºæœ‰ç¯çš„space graphï¼Œtreeæ˜¯æ— ç©·å¤§çš„ã€‚
fringeï¼ˆè¾¹ç•Œ / frontierï¼‰å­˜å‚¨çš„æ˜¯â€œå½“å‰æ­£åœ¨ç­‰å¾…æ‰©å±•çš„èŠ‚ç‚¹â€ï¼Œå‰é¢å·²ç»å±•å¼€ä¹‹åå°±remove.
1. 
![[Pasted image 20250919114233.png]]
>æ¯ä¸€å±‚å±•å¼€æœ‰bç§é€‰æ‹©ï¼Œmå±‚ï¼Œæœ€ç»ˆO(bm).è¿™æ ·çš„ç©ºé—´å¤æ‚åº¦æ¯”è¾ƒæœ‰ä¼˜åŠ¿

2. 
![[Pasted image 20250919113847.png]]
èƒ½æå‰åœ¨ç»ˆæ­¢ç‚¹ç»“æŸï¼Œä½†æ˜¯åœ¨æ‰¾åˆ°æœ€ç»ˆèŠ‚ç‚¹ä¹‹å‰è¦ä¿ç•™æ‰€æœ‰èŠ‚ç‚¹ã€‚
> å¯¹äºæµ…å±‚çš„è·¯å¾„ï¼ŒBFSæ›´æœ‰ä¼˜åŠ¿ã€‚

å¦‚æœè¯´è·¯å¾„éƒ½æ˜¯1ï¼Œå¯ä»¥ç»“åˆDFSå’ŒBFSï¼š
ï‚§ Idea: get DFSâ€™s space advantage with BFSâ€™s time / shallow-solution advantages ï‚§ Run a DFS with depth limit 1. If no solutionâ€¦ 
ï‚§ Run a DFS with depth limit 2. If no solutionâ€¦ 
ï‚§ Run a DFS with depth limit 3. â€¦.. 
ï‚§ Isnâ€™t that wastefully redundant? 
ï‚§ Generally most work happens in the lowest level searched, so not so bad!


3. UCS(ç±»ä¼¼äºdij)
![[Pasted image 20250921100433.png]]
4. A*

# Constraint Satisfaction Problems

Unlike search problems, CSPs are a type ofÂ **identification problem**, problems in which we must simply identify whether a state is a goal state or not, with no regard to how we arrive at that goal. CSPs are defined by three factors:
![[Pasted image 20251010102034.png]]
**NP-hard**
N variables with domain size{d}=>O(d^N)
- Partial assignment: variable assignments to CSPs where some variables have been assigned values while others have not
- Goal: all variables are assigned and all constraints are satisfied in the state itâ€™s testing.
- Constraint Graph:
- ![[Pasted image 20250925164426.png]]
> Coloring Problem

## **backtracking search**:
>1. Fix an ordering for variables, and select values for variables in this order. Because assignments are commutative (e.g. assigningÂ WA=Red,NT=GreenÂ is identical toÂ NT=Green,WA=Red), this is valid.
>2. . When selecting values for a variable, only select values that donâ€™t conflict with any previously assigned values. If no such values exist, backtrack and return to the previous variable, changing its value.
![[Pasted image 20250925164737.png]]
## improvement for bk:
  ### filtering:
   forward checking:A naÃ¯ve method for filtering isÂ **forward checking**, which whenever a value is assigned to a variableÂ Xi, prunes the domains of unassigned variables that share a constraint withÂ XiÂ that would violate the constraint if assigned.
	![[Pasted image 20250925165315.png]]
	- arc consistency: 
	1. åˆå§‹åŒ–é˜Ÿåˆ— Q
	- æŠŠæ‰€æœ‰å˜é‡ä¹‹é—´çš„çº¦æŸå…³ç³»ï¼ˆå¼§ï¼‰åŠ å…¥é˜Ÿåˆ— Qã€‚
	- æ¯ä¸ªçº¦æŸå…³ç³»æ˜¯ä¸€ä¸ªæ–¹å‘çš„å¼§ï¼Œæ¯”å¦‚ $$Xiâ†’XjX_i \rightarrow X_j$$
	2. è¿­ä»£å¤„ç†é˜Ÿåˆ—
	- ä» Q ä¸­å–å‡ºä¸€ä¸ªå¼§$$X_i \rightarrow X_j$$
	- æ£€æŸ¥ X_i çš„æ¯ä¸ªå€¼ vï¼šæ˜¯å¦å­˜åœ¨ X_j çš„æŸä¸ªå€¼ wï¼Œä½¿å¾— vv å’Œ ww ä¸è¿åçº¦æŸã€‚
		- å¦‚æœæ²¡æœ‰è¿™æ ·çš„ wwï¼Œå°±æŠŠ vv ä» XiX_i çš„åŸŸä¸­åˆ é™¤ã€‚
3. å¦‚æœæœ‰å€¼è¢«åˆ é™¤
	- å¦‚æœåœ¨ä¸Šä¸€æ­¥ä¸­ä» XiX_i çš„åŸŸä¸­åˆ é™¤äº†æŸäº›å€¼ï¼š
		- æŠŠæ‰€æœ‰æŒ‡å‘ XiX_i çš„å¼§ Xkâ†’XiX_k \rightarrow X_i åŠ å…¥é˜Ÿåˆ— Qï¼ˆé™¤éå·²ç»åœ¨é˜Ÿåˆ—ä¸­ï¼‰ã€‚
		- è¿™æ ·å¯ä»¥é‡æ–°æ£€æŸ¥è¿™äº›å˜é‡æ˜¯å¦ä»ç„¶æœ‰åˆæ³•å–å€¼ã€‚
	#### 4. ç»ˆæ­¢æ¡ä»¶
	- å½“é˜Ÿåˆ— Q ä¸ºç©ºæ—¶ï¼Œç®—æ³•ç»“æŸã€‚
	- å¦‚æœæŸä¸ªå˜é‡çš„åŸŸå˜æˆç©ºé›†ï¼Œè¯´æ˜æ²¡æœ‰åˆæ³•è§£ï¼Œè§¦å‘å›æº¯ã€‚
	![[Pasted image 20250925170440.png]]

> The AC-3 algorithm has a worst-case time complexity ofÂ O(ed3), whereÂ eÂ is the number of arcs (directed edges) andÂ dÂ is the size of the largest domain. Overall, arc consistency is more holistic of a domain pruning technique than forward checking and leads to fewer backtracks, but requires running significantly more computation in order to enforce.
ä»€ä¹ˆæ˜¯arc consistencyï¼šå¦‚æœå¯¹äºå˜é‡ **X** çš„æ¯ä¸€ä¸ªå¯èƒ½å–å€¼ **x**ï¼Œéƒ½å­˜åœ¨å˜é‡ **Y** çš„æŸä¸ªå–å€¼ **y**ï¼Œä½¿å¾— **(x, y)** æ»¡è¶³å®ƒä»¬ä¹‹é—´çš„çº¦æŸé‚£ä¹ˆæˆ‘ä»¬å°±è¯´ï¼š**å˜é‡ X å¯¹å˜é‡ Y æ˜¯å¼§ä¸€è‡´çš„ï¼ˆarc-consistentï¼‰**ã€‚
>This idea can be further extended through the idea ofÂ **strong k-consistency**. A graph that is strongÂ k-consistent possesses the property that any subset ofÂ kÂ nodes is not onlyÂ k-consistent but alsoÂ kâˆ’1,kâˆ’2,â€¦,1-consistent as well. Not surprisingly, imposing a higher degree of consistency on a CSP is more expensive to compute. Under this generalized definition for consistency, we can see that arc consistency is equivalent toÂ 2-consistency.å¯¹äºä»»æ„ **Kä¸ªå˜é‡**ï¼Œå¦‚æœä½ å·²ç»ä¸ºå…¶ä¸­çš„ **Kâˆ’1ä¸ªå˜é‡**èµ‹å€¼ï¼Œå¹¶ä¸”è¿™äº›èµ‹å€¼æ»¡è¶³æ‰€æœ‰çº¦æŸï¼Œé‚£ä¹ˆä½ ä¸€å®šå¯ä»¥ä¸ºç¬¬Kä¸ªå˜é‡æ‰¾åˆ°ä¸€ä¸ªå€¼ï¼Œä½¿å¾—æ•´ä¸ªKä¸ªå˜é‡çš„èµ‹å€¼ä»ç„¶æ»¡è¶³æ‰€æœ‰çº¦æŸã€‚


### orderding
#### _Minimum Remaining Values (MRV)_:
When selecting which variable to assign next, using an MRV policy chooses whichever unassigned variable has the fewest valid remaining values (theÂ _most constrained variable_). This is intuitive in the sense that the most constrained variable is most likely to run out of possible values and result in backtracking if left unassigned, and so itâ€™s best to assign a value to it sooner than later.
#### _Least Constraining Value (LCV)_:
Similarly, when selecting which value to assign next, a good policy to implement is to select the value that prunes the fewest values from the domains of the remaining unassigned values. Notably, **this requires additional computation** (e.g. rerunning arc consistency/forward checking or other filtering methods for each value to find the LCV), but can still yield speed gains depending on usage.

#### Tree-structured CSP
å¦‚æœä¸€ä¸ª CSP çš„çº¦æŸå›¾æ˜¯æ ‘çŠ¶ç»“æ„ï¼ˆæ²¡æœ‰ç¯ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªé«˜æ•ˆç®—æ³•åœ¨ **O(nÂ·dÂ²)** æ—¶é—´å†…è§£å†³å®ƒï¼Œå…¶ä¸­ï¼š
- nï¼šå˜é‡æ•°é‡
- dï¼šæ¯ä¸ªå˜é‡çš„å¯èƒ½å–å€¼æ•°é‡ï¼ˆåŸŸå¤§å°ï¼‰
1. é€‰ä¸€ä¸ªæ ¹èŠ‚ç‚¹

- ä»çº¦æŸå›¾ä¸­ä»»é€‰ä¸€ä¸ªå˜é‡ä½œä¸ºæ ¹èŠ‚ç‚¹ï¼ˆå› ä¸ºæ ‘çš„ä»»æ„èŠ‚ç‚¹éƒ½å¯ä»¥ä½œä¸ºæ ¹ï¼‰ã€‚
    
- å°†æ‰€æœ‰æ— å‘è¾¹è½¬æ¢ä¸º**ä»æ ¹èŠ‚ç‚¹å‡ºå‘çš„æœ‰å‘è¾¹**ï¼Œå½¢æˆä¸€ä¸ª**æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰**ã€‚
    

2. å¯¹å›¾è¿›è¡Œçº¿æ€§æ’åºï¼ˆæ‹“æ‰‘æ’åºï¼‰

- æ’åºåçš„å›¾ä¸­ï¼Œæ‰€æœ‰è¾¹éƒ½ä»å·¦æŒ‡å‘å³ï¼Œè¡¨ç¤ºä»çˆ¶èŠ‚ç‚¹åˆ°å­èŠ‚ç‚¹çš„ä¾èµ–å…³ç³»ã€‚
    

3. **åå‘éå†ï¼šè¿›è¡Œå¼§ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆBackward Arc Consistencyï¼‰**

ä»æœ€åä¸€ä¸ªå˜é‡ XnX_n å¼€å§‹ï¼Œå‘å‰éå†åˆ° X2X_2ï¼Œå¯¹æ¯ä¸ªå˜é‡æ‰§è¡Œï¼š

- æ£€æŸ¥å…¶çˆ¶èŠ‚ç‚¹åˆ°å®ƒçš„çº¦æŸï¼ˆå³ $Parent(X_i)â†’X_i\text{Parent}(X_i) \rightarrow X_iï¼‰$ã€‚
    
- å¯¹æ¯ä¸ªå˜é‡çš„å–å€¼åŸŸè¿›è¡Œ**å‰ªæ**ï¼šç§»é™¤é‚£äº›æ— æ³•ä¸çˆ¶èŠ‚ç‚¹å€¼å…¼å®¹çš„å–å€¼ã€‚
    

è¿™ä¸€æ­¥ç¡®ä¿äº†ï¼šæ¯ä¸ªå˜é‡çš„å–å€¼éƒ½èƒ½ä¸å…¶çˆ¶èŠ‚ç‚¹ä¿æŒä¸€è‡´ã€‚

4. **æ­£å‘éå†ï¼šè¿›è¡Œèµ‹å€¼ï¼ˆForward Assignmentï¼‰**

ä»æ ¹èŠ‚ç‚¹ X1X_1 å¼€å§‹ï¼Œä¾æ¬¡ä¸ºæ¯ä¸ªå˜é‡èµ‹å€¼ï¼š

- æ¯æ¬¡é€‰æ‹©ä¸€ä¸ªä¸çˆ¶èŠ‚ç‚¹å€¼å…¼å®¹çš„å–å€¼ã€‚
    
- å› ä¸ºä¹‹å‰å·²ç»åšäº†å¼§ä¸€è‡´æ€§æ£€æŸ¥ï¼Œä¿è¯äº†æ¯ä¸ªå˜é‡è‡³å°‘æœ‰ä¸€ä¸ªåˆæ³•å€¼ã€‚
    

è¿™ä¸ªè¿‡ç¨‹æ˜¯**çº¿æ€§çš„**ï¼Œä¸ä¼šå›æº¯ï¼Œä¹Ÿä¸ä¼šå†²çªï¼Œèƒ½ä¿è¯æ‰¾åˆ°ä¸€ä¸ªå®Œæ•´çš„è§£ã€‚

- Cutset Conditioning
ç°å®ä¸­çš„ CSP é€šå¸¸ä¸æ˜¯æ ‘çŠ¶å›¾ï¼Œè€Œæ˜¯æœ‰ç¯çš„å›¾ã€‚Cutset Conditioning çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

> æ‰¾å‡ºä¸€ä¸ªæœ€å°çš„å˜é‡é›†åˆï¼ˆç§°ä¸º **cutset**ï¼‰ï¼ŒæŠŠå®ƒä»¬ä»å›¾ä¸­â€œæ‹¿æ‰â€ï¼Œè®©å‰©ä¸‹çš„å›¾å˜æˆæ ‘ç»“æ„ã€‚

1. **æ‰¾åˆ°æœ€å° cutset**ï¼šè®¾è¿™ä¸ªé›†åˆå¤§å°ä¸º ccï¼Œä¾‹å¦‚åœ°å›¾ç€è‰²é—®é¢˜ä¸­ï¼ŒSouth Australia æ˜¯ä¸€ä¸ª cutsetã€‚
    
2. **æšä¸¾ cutset çš„æ‰€æœ‰å¯èƒ½èµ‹å€¼**ï¼šå› ä¸º cutset ä¸­çš„å˜é‡å¯èƒ½äº’ç›¸æœ‰çº¦æŸï¼Œæˆ‘ä»¬éœ€è¦å°è¯•æ‰€æœ‰å¯èƒ½çš„ç»„åˆï¼Œæœ€å¤šæœ‰ dcd^c ç§ã€‚
    
3. **æ¯æ¬¡å°è¯•ä¸€ä¸ª cutset èµ‹å€¼**ï¼š
    
    - å¯¹ cutset çš„é‚»å±…è¿›è¡Œ **åŸŸå‰ªæ**ï¼ˆprune domainsï¼‰ï¼Œæ’é™¤ä¸åˆæ³•çš„å–å€¼ã€‚
        
    - å‰©ä¸‹çš„ CSP æ˜¯æ ‘ç»“æ„ï¼Œå¯ä»¥ç”¨æ ‘ç»“æ„ç®—æ³•åœ¨ O((nâˆ’c)â‹…d2)O((n - c)Â·dÂ²) æ—¶é—´å†…è§£å†³ã€‚
        
4. **å¦‚æœæŸä¸ª cutset èµ‹å€¼å¯¼è‡´æ— è§£**ï¼Œå°±å›æº¯ï¼Œå°è¯•ä¸‹ä¸€ä¸ªç»„åˆã€‚
- cutset æœ‰ cc ä¸ªå˜é‡ï¼Œæ¯ä¸ªå˜é‡æœ€å¤šæœ‰ dd ä¸ªå€¼ â†’ æ€»å…±è¦å°è¯• dcd^c ç§ç»„åˆã€‚
    
- æ¯æ¬¡ç»„åˆéƒ½è¦è§£å†³ä¸€ä¸ªå¤§å°ä¸º nâˆ’cn - c çš„æ ‘ç»“æ„ CSP â†’ æ¯æ¬¡èŠ±è´¹ O((nâˆ’c)â‹…d2)O((n - c)Â·dÂ²)
    

æ‰€ä»¥æ€»æ—¶é—´å¤æ‚åº¦æ˜¯ï¼š

O(dcâ‹…(nâˆ’c)â‹…d2)



## Local Search
Local search works by iterative improvementâ€”start with some random assignment to values, then iteratively select a random conflicted variable and reassign its value to the one that violates the fewest constraints until no more constraint violations exist (a policy known as theÂ **min-conflicts heuristic**).

However, despite these advantages, local search is both incomplete and suboptimal and so wonâ€™t necessarily converge to an optimal solution. Additionally, there is a critical ratio around which using local search becomes extremely expensive:

![[Pasted image 20250925173714.png]]


# GAME
Deterministic Games:![[Pasted image 20251010105413.png]]
Zero-Sum Games: ä¸€æ–¹æœ€å¤§åŒ–æŸä¸ªå€¼ï¼Œå¦ä¸€æ–¹æƒ³æœ€å°åŒ–è¿™ä¸ªå€¼
General Gamesï¼šæ¯ä¸ªagentçš„å¾—åˆ†æ˜¯independentçš„

## Minmax
The first zero-sum-game algorithm we will consider isÂ **minimax**, which runs under the motivating assumption that the opponent we faceÂ behaves optimally, and will always perform the move that is worst for usÂ .
- AÂ **stateâ€™s value**Â is defined as theÂ best possible outcome (utility)Â an agent can achieve from that state[3](https://kg.darstib.cn/note/cs188/note/05-Trees_Minimax_Pruning/#fn:2).
- The value of aÂ **terminal state**, called a terminal utility, is always someÂ deterministic known value and an inherent game propertyÂ .
- In this example, the value of aÂ **non-terminal state**Â is defined as the maximum of the values of its children.
> the minimax algorithm only maximizes over the children of nodes controlled by Pacman, while minimizing over the children of nodes controlled by ghosts.
![[Pasted image 20250925175950.png]]
![[Pasted image 20250925180523.png]]
æœ€å°åŒ–é¬¼çš„èŠ‚ç‚¹ï¼ˆåˆ†åˆ«-8ï¼Œ-10ï¼‰ï¼Œæœ€å¤§åŒ–pacmanèŠ‚ç‚¹ï¼Œ-8

![[Pasted image 20251010113049.png]]

> Recalling that b is the branching factor and m is the approximate tree depth at which terminal nodes can be found, this yields far too great a runtime for many games. For example, chess has a branching factor b â‰ˆ 35 and tree depth m â‰ˆ 100.


## Alpha-Beta Pruning
Conceptually,Â **alpha-beta pruning**Â is this: if youâ€™re trying to determine the value of a node n by looking at its successors, stop looking as soon as you know that nâ€™s value can at best equal the optimal value of nâ€™s parent.
ä¾‹å­ï¼š![[Pasted image 20250925181122.png]]
![[Pasted image 20250925181131.png]]
ä¼ªä»£ç ï¼š
![[Pasted image 20250925181217.png]]
## Expectimax
Because minimax believes it is responding to an optimal opponent, itâ€™s often **overly pessimistic** in situations where optimal responses to an agentâ€™s actions are not guaranteed. Such situations include scenarios with inherent randomness such as card or dice games or unpredictable opponents that move randomly or suboptimally. This randomness can be represented through a generalization of minimax known asÂ **expectimax**.
Expectimax introducesÂ **chance nodes**Â into the game tree, which instead of considering the worst case scenario as minimizer nodes do, considers theÂ **average case**. More specifically, while minimizers simply compute the minimum utility over their children, chance nodes compute the expected utility or expected value.
åœ¨ **Expectimax** é‡Œï¼Œ**Pacmanï¼ˆagentIndex = 0ï¼‰** åº”è¯¥æ˜¯ **max èŠ‚ç‚¹**ï¼Œé€‰æ‹©èƒ½è®©è‡ªå·±çš„åˆ†æ•°æœ€å¤§çš„åŠ¨ä½œï¼›è€Œ **é¬¼ï¼ˆagentIndex > 0ï¼‰** æ˜¯ **chance èŠ‚ç‚¹**ï¼Œå–åˆæ³•åŠ¨ä½œçš„å‡å€¼ã€‚
![[Pasted image 20250925205424.png]]


### Mixed Layer Types
In the Pacman example for a game with four ghosts, this can be done by having a maximizer layer followed by 4 consecutive ghost/minimizer layers before the second Pacman/maximizer layer.
![[Pasted image 20250925205914.png]]

- Expectiminimax:Environment is an extra â€œrandom agentâ€ player that moves after each min/max agent.ä¹Ÿå°±æ˜¯å¯¹æŠ—ghostä¸‹ä¸€ä¸ªghostæ˜¯random ghost

### Multi-Agent Utilities
![[Pasted image 20251011113115.png]]
- Utilities:ååº”æœºåˆ¶![[Pasted image 20251011113508.png]]
> å¯¹äºminmaxï¼Œminmaxç®—æ³•ä¸å—å•è°ƒçš„å˜åŒ–ï¼Œä½†æ˜¯expctimaxæ˜¯å—è¿™ç§å½±å“çš„ã€‚


### Monte Carlo Tree Search

1. **Evaluation by rolloutsï¼ˆé€šè¿‡æ¨¡æ‹Ÿè¯„ä¼°ï¼‰**

- ä»å½“å‰çŠ¶æ€ ss å¼€å§‹ï¼Œä½¿ç”¨ä¸€ä¸ªç­–ç•¥ï¼ˆæ¯”å¦‚éšæœºèµ°å­ï¼‰è¿›è¡Œå¤šæ¬¡å®Œæ•´çš„æ¸¸æˆæ¨¡æ‹Ÿï¼Œç›´åˆ°æ¸¸æˆç»“æŸã€‚
- æ¯æ¬¡æ¨¡æ‹Ÿè®°å½•ç»“æœï¼ˆèƒœ/è´Ÿï¼‰ï¼Œç»Ÿè®¡èƒœç‡ã€‚
- èƒœç‡è¶Šé«˜ï¼Œè¯´æ˜è¿™ä¸ªçŠ¶æ€è¶Šâ€œæœ‰åˆ©â€ã€‚

è¿™å°±åƒæ˜¯ä½ åœ¨æŸä¸ªå±€é¢ä¸‹ï¼Œ**è¯•ç€ç©å¾ˆå¤šæ¬¡çœ‹çœ‹ç»“æœå¦‚ä½•**ï¼Œä»è€Œä¼°è®¡è¿™ä¸ªå±€é¢çš„ä»·å€¼ã€‚

2. **Selective searchï¼ˆé€‰æ‹©æ€§æœç´¢ï¼‰**
- ä¸å»æ¢ç´¢æ•´ä¸ªæ¸¸æˆæ ‘ï¼Œè€Œæ˜¯**é‡ç‚¹æ¢ç´¢é‚£äº›çœ‹èµ·æ¥æ›´æœ‰å¸Œæœ›çš„åˆ†æ”¯**ã€‚
- æ¯æ¬¡æ¨¡æ‹Ÿåï¼Œæ ¹æ®ç»“æœæ›´æ–°æ ‘çš„ç»“æ„ï¼Œä¼˜å…ˆæ‰©å±•é‚£äº›èƒœç‡é«˜çš„èŠ‚ç‚¹ã€‚
- æ²¡æœ‰å›ºå®šçš„æœç´¢æ·±åº¦é™åˆ¶ï¼ˆâ€œæ²¡æœ‰ horizon constraintâ€ï¼‰ï¼Œå¯ä»¥æ ¹æ®éœ€è¦æ·±å…¥æŸäº›åˆ†æ”¯ã€‚

è¿™å°±åƒä½ åœ¨ç©æ¸¸æˆæ—¶ï¼Œ**æ›´å…³æ³¨é‚£äº›ä½ è§‰å¾—å¯èƒ½èµ¢çš„ç­–ç•¥**ï¼Œè€Œä¸æ˜¯å¹³å‡åˆ†é…æ³¨æ„åŠ›ã€‚
## Evaluation Functions
Evaluation functions are widely employed in depth-limited minimax, where we treat non-terminal nodes located at our maximum solvable depth as terminal nodes, giving them mock terminal utilities as determined by a carefully selected evaluation function. 
Because evaluation functions can only yield estimates of the values of non-terminal utilities, this removes the guarantee of optimal play when running minimax.
æœ‰äº›æƒ…å†µå‰ªæåä¾æ—§éš¾ä»¥è®¡ç®—Â =>Â å¼ºè¡Œæˆªæ–­Â game treeï¼Œå…¶æœ€åº•å±‚çš„Â non-terminal statesÂ ç”±Â evaluate functionÂ èµ‹äºˆå…¶Â knownÂ è€Œå˜ä¸ºÂ terminal states =>Â é™ä½äº†æ ‘çš„æ·±åº¦å’Œå…¶å¯é æ€§ã€‚

Â Additionally, going deeper into the tree before using an evaluation function also tends to give us better resultsÂ - burying their computation deeper in the game tree mitigates the compromising of optimality.
![[Pasted image 20250925181609.png]]


# Propositional Logic
- conjunction
- disjunction
- implication
- biconditional
- $S1\ \rightarrow\ S2$ iff S1 false or S2 true
- **Validity:** valid in all models $A \vee A$ å¦‚æœå¯¹äºä»»æ„ä¸€ç§å˜é‡èµ‹å€¼æ–¹å¼ï¼Œä½¿å¾—æ•´ä¸ªå…¬å¼ä¸ºçœŸã€‚
- knowledge based agentï¼šSuch an agent maintains a knowledge base, which isÂ a **collection of logical sentences that encodes what we have told the agent and what it has observed**
![[Pasted image 20251006164927.png]]
- **Satisfiability**ï¼šåœ¨**å‘½é¢˜é€»è¾‘**ä¸­ï¼Œç»™å®šä¸€ä¸ªé€»è¾‘å…¬å¼ï¼Œæˆ‘ä»¬è¯´å®ƒæ˜¯â€œ**å¯æ»¡è¶³çš„ï¼ˆsatisfiableï¼‰**â€ï¼Œå¦‚æœå­˜åœ¨ä¸€ç§å˜é‡èµ‹å€¼æ–¹å¼ï¼Œä½¿å¾—æ•´ä¸ªå…¬å¼ä¸ºçœŸã€‚
> - $A\Rightarrow  B \equiv \neg A\vee B$:
> - A|=B éœ€è¦è¯æ˜$A\and \neg B$ is unsatisfiable:å¦‚æœæˆ‘ä»¬ä»¤ $  $ï¼Œé‚£ä¹ˆæ•´ä¸ªå…¬å¼ä¸ºçœŸ â†’ âœ… å¯æ»¡è¶³

## Horn logic
ä¸ºäº†è®©æ¨ç†æ›´é«˜æ•ˆï¼Œ![[image-9.png]]
![[image-10.png]]
# Firstorder Logic
| å…ƒç´         | è¯´æ˜                                                |
| --------- | ------------------------------------------------- |
| **å¸¸é‡ç¬¦å·**  | è¡¨ç¤ºå…·ä½“çš„å¯¹è±¡ï¼Œå¦‚ `John`ã€`Shanghai`                       |
| **å˜é‡ç¬¦å·**  | è¡¨ç¤ºä»»æ„å¯¹è±¡ï¼Œå¦‚ `x`ã€`y`                                  |
| **è°“è¯ç¬¦å·**  | è¡¨ç¤ºå¯¹è±¡ä¹‹é—´çš„å…³ç³»æˆ–å±æ€§ï¼Œå¦‚ `Loves(John, Mary)` è¡¨ç¤º John çˆ± Mary |
| **å‡½æ•°ç¬¦å·**  | è¡¨ç¤ºå¯¹è±¡ä¹‹é—´çš„æ˜ å°„ï¼Œå¦‚ `FatherOf(x)` è¡¨ç¤º x çš„çˆ¶äº²                |
| **é‡è¯**    | ç”¨äºè¡¨è¾¾èŒƒå›´ï¼š`âˆ€x` è¡¨ç¤ºâ€œå¯¹æ‰€æœ‰ xâ€ï¼Œ`âˆƒx` è¡¨ç¤ºâ€œå­˜åœ¨æŸä¸ª xâ€             |
| **é€»è¾‘è¿æ¥è¯** | å¦‚ âˆ§ï¼ˆä¸ï¼‰ã€âˆ¨ï¼ˆæˆ–ï¼‰ã€Â¬ï¼ˆéï¼‰ã€â‡’ï¼ˆè•´å«ï¼‰ç­‰                           |
å‡è®¾æˆ‘ä»¬è¦è¡¨è¾¾â€œæ‰€æœ‰äººéƒ½æœ‰æ¯äº²â€ï¼š

- ç”¨ä¸€é˜¶é€»è¾‘å¯ä»¥å†™æˆï¼š $âˆ€xÂ Person(x) â‡’ âˆƒy\ Mother(y, x)$ è¿™è¡¨ç¤ºï¼šå¯¹æ‰€æœ‰ xï¼Œå¦‚æœ x æ˜¯äººï¼Œé‚£ä¹ˆå­˜åœ¨ä¸€ä¸ª yï¼Œä½¿å¾— y æ˜¯ x çš„æ¯äº²ã€‚

å†æ¯”å¦‚ï¼Œâ€œJohn æ˜¯ Mary çš„å…„å¼Ÿâ€å¯ä»¥å†™æˆï¼š $Brother(John,Mary)$

SAT çš„åŸºæœ¬å®šä¹‰

ç»™å®šä¸€ä¸ªç”±å¸ƒå°”å˜é‡ï¼ˆå¦‚ Aã€Bã€Cï¼‰å’Œé€»è¾‘è¿ç®—ç¬¦ï¼ˆå¦‚ âˆ§ã€âˆ¨ã€Â¬ï¼‰ç»„æˆçš„å…¬å¼ï¼ŒSAT é—®é¢˜å°±æ˜¯åˆ¤æ–­ï¼š

> æœ‰æ²¡æœ‰ä¸€ç§æ–¹å¼ç»™è¿™äº›å˜é‡èµ‹å€¼ï¼ˆTrue æˆ– Falseï¼‰ï¼Œä½¿æ•´ä¸ªå…¬å¼æˆç«‹ï¼Ÿ

å¦‚æœæœ‰ â†’ è¿™ä¸ªå…¬å¼æ˜¯ **å¯æ»¡è¶³çš„ï¼ˆsatisfiableï¼‰** å¦‚æœæ²¡æœ‰ â†’ è¿™ä¸ªå…¬å¼æ˜¯ **ä¸å¯æ»¡è¶³çš„ï¼ˆunsatisfiableï¼‰**
Inference algorithmï¼š
1. Forward Chainingï¼š
![[image-11.png]]
data-drivenã€‚ a-b-l-m-p-q
ä»€ä¹ˆæ˜¯ Forward Chainingï¼Ÿ

Forward Chaining æ˜¯ä¸€ç§**æ•°æ®é©±åŠ¨çš„æ¨ç†æ–¹æ³•**ï¼Œå®ƒä»å·²çŸ¥äº‹å®å‡ºå‘ï¼Œä¸æ–­åº”ç”¨è§„åˆ™ï¼Œæ¨å¯¼å‡ºæ–°çš„äº‹å®ï¼Œç›´åˆ°ï¼š
- æ¨å¯¼å‡ºæˆ‘ä»¬å…³å¿ƒçš„ç›®æ ‡å‘½é¢˜ q âœ…
- æˆ–è€…å†ä¹Ÿæ¨ä¸å‡ºæ–°çš„äº‹å® âŒ
 å·¥ä½œæµç¨‹è¯¦è§£
1. **åˆå§‹åŒ–**ï¼š
    - ä»çŸ¥è¯†åº“ï¼ˆKBï¼‰ä¸­æ‰¾å‡ºæ‰€æœ‰å·²çŸ¥ä¸ºçœŸçš„å‘½é¢˜ï¼ˆæ¯”å¦‚ A æ˜¯çœŸçš„ï¼‰
    - æŠŠè¿™äº›å‘½é¢˜æ”¾å…¥ä¸€ä¸ªé˜Ÿåˆ—ï¼ˆå«åš agendaï¼‰
2. **è¿­ä»£æ¨ç†**ï¼š
    - æ¯æ¬¡ä» agenda ä¸­å–å‡ºä¸€ä¸ªå·²çŸ¥ä¸ºçœŸçš„å‘½é¢˜ p
    - æ£€æŸ¥çŸ¥è¯†åº“ä¸­æœ‰å“ªäº›è§„åˆ™çš„å‰æåŒ…å« p
    - å¯¹è¿™äº›è§„åˆ™çš„å‰æè®¡æ•°å‡ä¸€ï¼ˆè¡¨ç¤ºæˆ‘ä»¬å·²ç»çŸ¥é“å…¶ä¸­ä¸€ä¸ªå‰æä¸ºçœŸï¼‰
    - å¦‚æœæŸæ¡è§„åˆ™çš„æ‰€æœ‰å‰æéƒ½å·²çŸ¥ä¸ºçœŸ â†’ é‚£ä¹ˆå®ƒçš„ç»“è®ºä¹Ÿä¸ºçœŸ â†’ åŠ å…¥ agenda
3. **ç»ˆæ­¢æ¡ä»¶**ï¼š
    - å¦‚æœæˆ‘ä»¬æ¨å¯¼å‡ºäº†ç›®æ ‡å‘½é¢˜ q â†’ è¿”å› true
    - å¦‚æœ agenda ä¸ºç©ºï¼Œè¯´æ˜æ— æ³•å†æ¨ç†å‡ºæ–°äº‹å® â†’ è¿”å› false
```
function PL-FC-ENTAILS?(KB, q) returns true or false  
    inputs:  KB, q

count â† table, where count[c] is the number of symbols in câ€™s premise  
inferred â† table, where inferred[s] is initially false for all symbols  
agenda â† a queue of symbols, initially symbols known to be true in KB  

while agenda is not empty do  
    p â† Pop(agenda)  # ä» agenda ä¸­å–å‡ºä¸€ä¸ªç¬¦å· p  
    if p = q then return true  # å¦‚æœ p æ˜¯æŸ¥è¯¢ qï¼Œè¿”å› true  
    if inferred[p] = false then  # å¦‚æœ p å°šæœªè¢«æ¨æ–­  
        inferred[p] â† true  # æ ‡è®° p ä¸ºå·²æ¨æ–­  
        for each clause c in KB where p is in c.Premise do  
            decrement count[c]  # å¯¹æ¯ä¸ªåŒ…å« p çš„å­å¥ cï¼Œå‡å°‘å…¶å‰æç¬¦å·è®¡æ•°  
            if count[c] = 0 then  # å¦‚æœ c çš„æ‰€æœ‰å‰æç¬¦å·éƒ½å·²è¢«æ¨æ–­  
                add c.Conclusion to agenda  # å°† c çš„ç»“è®ºæ·»åŠ åˆ° agenda  
return false  # å¦‚æœ agenda å¤„ç†å®Œæ¯•ä»æœªæ‰¾åˆ° qï¼Œè¿”å› false
```
1. Backeard chainingï¼š
![[image-12.png]]
goal-driven:q-p-l-m-a-b
# DPLL ForwardChaining
- entailmentï¼ˆè•´å«ï¼‰: $\alpha |= \beta$ where a true b true.
- rule of inference:

| æ¨ç†è§„åˆ™åç§°                            | å½¢å¼è¡¨è¾¾å¼                                      | è¯´æ˜                              |
| --------------------------------- | ------------------------------------------ | ------------------------------- |
| **è‚¯å®šå‰ä»¶ï¼ˆModus Ponensï¼‰**            | Pâ†’Q,â€…â€ŠPâŠ¢Q$P \rightarrow Q$, ; $P \vdash Q$ | å¦‚æœ P è•´å« Qï¼Œä¸” P ä¸ºçœŸï¼Œåˆ™ Q ä¸ºçœŸã€‚        |
| **å¦å®šåä»¶ï¼ˆModus Tollensï¼‰**           | Pâ†’Q,â€…â€ŠÂ¬QâŠ¢Â¬P                                | å¦‚æœ P è•´å« Qï¼Œä¸” Q ä¸ºå‡ï¼Œåˆ™ P ä¸ºå‡ã€‚        |
| **æå–ä¸‰æ®µè®ºï¼ˆDisjunctive Syllogismï¼‰**  | Pâˆ¨Q,â€…â€ŠÂ¬PâŠ¢Q                                 | å¦‚æœ P æˆ– Q ä¸ºçœŸï¼Œä¸” P ä¸ºå‡ï¼Œåˆ™ Q ä¸ºçœŸã€‚      |
| **å‡è¨€ä¸‰æ®µè®ºï¼ˆHypothetical Syllogismï¼‰** | Pâ†’Q,â€…â€ŠQâ†’RâŠ¢Pâ†’R                              | é“¾å¼æ¨ç†ï¼šå¦‚æœ P æ¨å‡º Qï¼ŒQ æ¨å‡º Rï¼Œåˆ™ P æ¨å‡º Rã€‚ |
| **æ„é€ æ€§ä¸¤éš¾ï¼ˆConstructive Dilemmaï¼‰**   | Pâ†’Q,â€…â€ŠRâ†’S,â€…â€ŠPâˆ¨RâŠ¢Qâˆ¨S                        | ä»ä¸¤ä¸ªæ¡ä»¶æ¨å‡ºä¸¤ä¸ªå¯èƒ½çš„ç»“è®ºã€‚                 |

- å¦‚æœæˆ‘ä»¬æƒ³åˆ¤æ–­ä¸€ä¸ªå‘½é¢˜ B æ˜¯å¦æ˜¯ç”±å‘½é¢˜ A æ¨ç†å‡ºæ¥çš„ï¼ˆå³ A âŠ¨ Bï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ„é€ ä¸€ä¸ªæ–°çš„å…¬å¼ï¼š**A âˆ§ Â¬B**ï¼Œç„¶åæ£€æŸ¥å®ƒæ˜¯å¦**ä¸å¯æ»¡è¶³ï¼ˆunsatisfiableï¼‰**ã€‚ä¹Ÿå°±æ˜¯SATæ‰¾ä¸åˆ°ä¸€ä¸ªæ¨¡å‹ä½¿å¾—è¿™ä¸ªè¡¨è¾¾å¼å¯æ»¡è¶³ï¼Œä¹Ÿå°±æ˜¯è¿™ä¸ªè¡¨è¾¾å¼æœ€ç»ˆèƒ½è¿”å›true
## DPLL
- DPLLè¦æ±‚è¾“å…¥æ˜¯CNFï¼Œå³å…¬å¼æ˜¯è‹¥å¹²ä¸ªå­å¥çš„â€œä¸â€è¿æ¥ï¼Œæ¯ä¸ªå­å¥æ˜¯è‹¥å¹²ä¸ªæ–‡å­—çš„â€œæˆ–â€è¿æ¥ã€‚ä¾‹å¦‚ï¼š$(Aâˆ¨B)âˆ§(Â¬Câˆ¨D)âˆ§(E)(A \lor B) \land (\neg C \lor D) \land (E)$Then DPLL will continue assigning symbols truth values until either a satisfying model is found or a symbol cannot be assigned without violating a logical constraint, at which point the algorithm will backtrack to the last working assignment
However, DPLL makes three improvements over simple backtracking search:

1. **Early Termination**: A clause is true if any of the symbols are true. Also, a sentence is false if any single clause is false.
2. **Pure Symbol Heuristic**: A pure symbol is a symbol that only shows up in its positive form (or only in its negative form) throughout the entire sentence. Pure symbols can immediately be assigned true or false.[7](https://kg.darstib.cn/note/cs188/note/08-DPLL%26ForwardChaining/#fn:3)For example, N is not a pure literal because the first clause uses the negated Â¬N, and the second clause uses the non-negated N.
3. **Unit Clause Heuristic**: A unit clause is a clause with just one literal or a disjunction with one literal and many falses. In a unit clause, we can immediately assign a value to the literal, since there is only one valid assignment.[8](https://kg.darstib.cn/note/cs188/note/08-DPLL%26ForwardChaining/#fn:4)
4. ä¼ªä»£ç ï¼š
```
function DPLL-SATISFIABLE?(s) returns true or false  
inputs: s, a sentence in propositional logic  
    # å°†è¾“å…¥çš„å‘½é¢˜é€»è¾‘å¥å­è½¬æ¢ä¸º CNF å½¢å¼çš„å­å¥é›†åˆ  
    clauses â† the set of clauses in the CNF representation of s  
    # è·å–æ‰€æœ‰çš„å‘½é¢˜ç¬¦å·  
    symbols â† a list of the proposition symbols in s  
     # è°ƒç”¨ DPLL ç®—æ³•è¿›è¡Œæ±‚è§£  
    return DPLL(clauses, symbols, {})
```

## Forward Chaining

A logical Pacman
![[image-13.png]]
1. Sensor Model![[image-14.png]]
2. Transition model![[image-15.png]]
3. initial state:![[image-16.png]]
4. domain constraint
- cannot be in two places at once
- cannot take two actions at the same time!
- cannot go into a wall!
planning:
â€¢ SAT solver
â€“ Input: a logic expression
â€“ Output: a model (true/false assignments to symbols) that satisfies the expression if such a model exists
â€¢ Can we use it to make plans for Pacman (e.g., to move to a goal position)?
â€“ For T = 1 to infinity, set up the KB as follows and run SAT solver: 
	â€¢ Initial state, domain constraints, sensor & transition model sentences up to time T 
	â€¢ Goal is true at time Tâ€“ If a model is returned, extract a plan from action assignment

è§£å†³


# Intro to Probability
## Inference By Enumeration (IBE)
Given a joint PDF[1](https://kg.darstib.cn/note/cs188/note/10-Intro_to_Probability/#fn:1), we can trivially compute any desired probability distribution $$P(Q_i...Q_m|e_i...e_n)$$ using a simple and intuitive procedure known asÂ **inference by enumeration**, for which we define three types of variables we will be dealing with
1. **Query variables**Â Â $Q_i$, which are unknown and appear on the left side of the conditional bar(|) in the desired probability distribution.
2. **Evidence variables** $e_i$Â Â , which are observed variables whose values are known and appear on the right side of the conditional bar(|) in the desired probability distribution.
3. **Hidden variables**, which are values present in the overall joint distribution but not in the desired distributio
![[image.png]]
## Bayes net
defineï¼š![[image-2.png]]
![[image-1.png]]
åˆ©ç”¨æ¡ä»¶ç‹¬ç«‹

## Variable Elimination

![[image-3.png]]
- **æ¶ˆé™¤** CC
    
    - æ‰¾å‡ºæ‰€æœ‰åŒ…å« C çš„å› å­ï¼š$P(Câˆ£T)å’Œ P(Eâˆ£C,S)$
    - å°†å®ƒä»¬ç›¸ä¹˜å½¢æˆæ–°å› å­ $f_1 = P(C|T) \cdot P(E|C,S)$
    - å¯¹ C æ±‚å’Œï¼ˆè¾¹ç¼˜åŒ–ï¼‰ï¼Œå¾—åˆ°æ–°å› å­ $f_2(+e, T, S)$
        
- **æ¶ˆé™¤** SS
    
    - å°† f2 ä¸ P(S|T) ç›¸ä¹˜ï¼Œå¾—åˆ° f3(+e, T, S)
        
    - å¯¹ S æ±‚å’Œï¼Œå¾—åˆ° f4(+e,T)
        
- **ç»“åˆå‰©ä½™å› å­**
    
    - å°† f4(+e,T)ä¸ P(T)ç›¸ä¹˜ï¼Œå¾—åˆ° f5(+e,T)
        
- **å½’ä¸€åŒ–**
    
    - æœ€åå¯¹ f5(+e,T)f_5(+e, T) è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¾—åˆ°æˆ‘ä»¬æƒ³è¦çš„ P(Tâˆ£+e)
    - 


![[image-4.png]]![[image-5.png]]
ç¬¬ä¸€ä¸ªå…¬å¼å³è¾¹å¾—åˆ°çš„æ˜¯$P(+e,T)$

## Approximate Inference
é€šè¿‡é‡‡æ ·ä¼°è®¡
### Prior Sampling
exampleï¼š

```python
import random

def get_t():
    if random.random() < 0.99:
        return True
    return False

def get_c(t):
    if t and random.random() < 0.95:
        return True
    return False

def get_sample():
    t = get_t()
    c = get_c(t)
    return [t, c]
```

![[image-6.png]]
é—®é¢˜æ˜¯ï¼šå¯¹äºä¸€äº›æç«¯ä¾‹å­éœ€è¦99%çš„é‡‡æ ·ã€‚Â If we wanted to compute$P(C|-t)$Â , weâ€™d have to throw away 99% of our samples.
### **Rejection** Samplingï¼š
if we want to generate P(C|-t), we don't generate c if T=+t so we can throw most of bad samples to faster generating.
å¦‚æœä½ ç›´æ¥**å¼ºåˆ¶è®¾ç½®å˜é‡** t=falset = \text{false}ï¼Œä½ å¯èƒ½ä¼šå¾—åˆ°ä¸€äº›**ä¸ç¬¦åˆçœŸå®è”åˆåˆ†å¸ƒ**çš„æ ·æœ¬ã€‚ä¸ºä»€ä¹ˆï¼Ÿ

å› ä¸ºä½ è·³è¿‡äº†å¯¹ tt çš„é‡‡æ ·è¿‡ç¨‹ï¼Œå¯¼è‡´æ•´ä¸ªæ ·æœ¬çš„æ¦‚ç‡åªç”±å…¶ä»–å˜é‡çš„æ¡ä»¶æ¦‚ç‡å†³å®šï¼Œè€Œä¸æ˜¯å®Œæ•´çš„è”åˆåˆ†å¸ƒã€‚

**æ ¸å¿ƒæ€æƒ³ï¼š** è™½ç„¶æˆ‘ä»¬å¼ºåˆ¶è®¾ç½®äº†è¯æ®å˜é‡ï¼Œä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡ç»™æ¯ä¸ªæ ·æœ¬ä¸€ä¸ªâ€œæƒé‡â€æ¥è¡¥å¿è¿™ä¸ªåå·®ã€‚

è¿™ä¸ªæƒé‡æ˜¯ï¼š
$\text{Weight} = P(\text{evidence} \mid \text{sampled variables})$
ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯ä¸ªæ ·æœ¬çš„æƒé‡æ˜¯ï¼š**åœ¨è¯¥æ ·æœ¬ä¸‹ï¼Œè¯æ®å‡ºç°çš„æ¦‚ç‡**ã€‚

è¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼š
- æˆ‘ä»¬ä»ç„¶å¯ä»¥å¼ºåˆ¶è®¾ç½®è¯æ®å˜é‡ï¼ˆé¿å…ç”Ÿæˆæ— æ•ˆæ ·æœ¬ï¼‰ã€‚
- ä½†é€šè¿‡åŠ æƒï¼Œæˆ‘ä»¬æ¢å¤äº†å¯¹çœŸå®åˆ†å¸ƒçš„è¿‘ä¼¼ã€‚
- æ‰€æœ‰æ ·æœ¬çš„è´¡çŒ®ç”±å…¶æƒé‡å†³å®šï¼Œæƒé‡é«˜çš„æ ·æœ¬å½±å“æ›´å¤§ã€‚

```python
def likelihood_weighting(X, e, bn, N):
    """
    Calculate the posterior probability P(X | e) for the query variable X given evidence e.

    Parameters:
    X: Query variable
    e: Observed values for evidence variables
    bn: Bayesian network specifying the joint distribution P(X1, ..., Xn)
    N: Total number of samples to be generated

    Returns:
    Normalized weight vector W
    """
    W = {value: 0 for value in X.values()}  # Initialize weight vector

    for j in range(N):
        x, w = weighted_sample(bn, e)  # Generate sample and weight
        W[x] += w  # Update weight

    return normalize(W)  # Normalize weights

def weighted_sample(bn, e):
    """
    Generate an event and a weight.

    Parameters:
    bn: Bayesian network
    e: Observed values for evidence variables

    Returns:
    x: Generated event
    w: Weight
    """
    w = 1  # Initialize weight to 1
    x = {}  # Initialize event

    for Xi in bn.variables:  # Iterate over all variables
        if Xi in e:  # If it is an evidence variable
            w *= P(Xi | parents(Xi))  # Update weight
            x[Xi] = e[Xi]  # Set event value to the evidence value
        else:
            x[Xi] = random_sample(P(Xi | parents(Xi)))  # Sample from the conditional distribution

    return x, w  # Return event and weight

def normalize(W):
    """
    Normalize the weight vector.

    Parameters:
    W: Weight vector

    Returns:
    Normalized weight vector
    """
    total_weight = sum(W.values())
    return {key: value / total_weight for key, value in W.items()}  # Normalize
```

### Gibbs sampling
In this approach, we first set all variables to some totally random value (not taking into account any CPTs). We then repeatedly pick one variable at a time, clear its value, and resample it given the values currently assigned to all other variables.
![[image-7.png]]
æ€»çš„æ¥è¯´æ¯æ­¤é‡‡æ ·éƒ½ä¼šæ ¹æ®å…¶ä»–éè¯æ®å˜é‡æ¥æ›´æ–°æŸä¸ªå˜é‡çš„æ¡ä»¶æ¦‚ç‡ï¼Œå¹¶ä¸”æ¡ä»¶å–æ¦‚ç‡æ›´å¤§çš„é‚£ä¸ªå€¼ã€‚åå¤æ“ä½œNæ¬¡ï¼Œ
ç®€å•çš„æ¨¡æ‹Ÿï¼š
- P(T=T)=0.2P(T = T) = 0.2, P(T=F)=0.8P(T = F) = 0.8
    
- P(S=Tâˆ£T=T)=0.7P(S = T | T = T) = 0.7, P(S=Tâˆ£T=F)=0.2P(S = T | T = F) = 0.2
    
- P(E=Tâˆ£T=T)=0.9P(E = T | T = T) = 0.9, P(E=Tâˆ£T=F)=0.3
ğŸ”§ åˆå§‹åŒ–ï¼š

- å›ºå®š E=TE = T
    
- éšæœºè®¾å®šåˆå§‹å€¼ï¼šT=FT = F, S=TS = T
    

ğŸŒ€ ç¬¬ 1 æ¬¡è¿­ä»£ï¼š

æ›´æ–° TTï¼š

è®¡ç®—ï¼š

P(Tâˆ£S=T,E=T)âˆP(T)â‹…P(S=Tâˆ£T)â‹…P(E=Tâˆ£T)P(T | S = T, E = T) âˆ P(T) \cdot P(S = T | T) \cdot P(E = T | T)
- T=TT = T: 0.2â‹…0.7â‹…0.9=0.1260.2 \cdot 0.7 \cdot 0.9 = 0.126
- T=FT = F: 0.8â‹…0.2â‹…0.3=0.0480.8 \cdot 0.2 \cdot 0.3 = 0.048

å½’ä¸€åŒ–ï¼š
- P(T=T)â‰ˆ0.724P(T = T) â‰ˆ 0.724, P(T=F)â‰ˆ0.276P(T = F) â‰ˆ 0.276
    
é‡‡æ ·ç»“æœï¼š**T = T**
 æ›´æ–° SSï¼š
- P(S=Tâˆ£T=T)=0.7P(S = T | T = T) = 0.7, P(S=Fâˆ£T=T)=0.3P(S = F | T = T) = 0.3 é‡‡æ ·ç»“æœï¼š**S = T**
è®°å½•ï¼šT=TT = T
ğŸŒ€ ç¬¬ 2 æ¬¡è¿­ä»£ï¼š
æ›´æ–° TTï¼š
- S=TS = T, E=TE = T
- åŒä¸Šï¼Œç»“æœä»æ˜¯ï¼šP(T=T)â‰ˆ0.724P(T = T) â‰ˆ 0.724
é‡‡æ ·ç»“æœï¼š**T = T**

#### æ›´æ–° SSï¼š

é‡‡æ ·ç»“æœï¼š**S = F**

è®°å½•ï¼šT=TT = T

 ğŸŒ€ ç¬¬ 3 æ¬¡è¿­ä»£ï¼š

æ›´æ–° TTï¼š

- S=FS = F, E=TE = T
    

è®¡ç®—ï¼š

- T=TT = T: 0.2â‹…0.3â‹…0.9=0.0540.2 \cdot 0.3 \cdot 0.9 = 0.054
    
- T=FT = F: 0.8â‹…0.8â‹…0.3=0.1920.8 \cdot 0.8 \cdot 0.3 = 0.192
    

å½’ä¸€åŒ–ï¼š

- P(T=T)â‰ˆ0.219P(T = T) â‰ˆ 0.219, P(T=F)â‰ˆ0.781P(T = F) â‰ˆ 0.781
    

é‡‡æ ·ç»“æœï¼š**T = F**

æ›´æ–° SSï¼š

- P(S=Tâˆ£T=F)=0.2P(S = T | T = F) = 0.2, P(S=Fâˆ£T=F)=0.8P(S = F | T = F) = 0.8 é‡‡æ ·ç»“æœï¼š**S = F**
    

è®°å½•ï¼šT=F

æœ€åæ ¹æ®5æ¬¡æ›´æ–°ä¸­Tå–å€¼æœ€å¤šçš„é‚£ä¸ªæ¥å–ã€‚


# Markov Models
weather example:
$$
P(W_0, W_1, \ldots, W_n) = P(W_0)P(W_1|W_0)P(W_2|W_0, W_1)\ldots P(W_n|W_{n-1}) = P(W_0) \prod_{i=0}^{n-1} P(W_{i+1}|W_i)

$$
To track how our quantity under consideration (in this case, the weather) changes over time, we need to know both itâ€™sÂ **initial distribution**Â at time t = 0 and some sort ofÂ **transition model**Â that characterizes the probability of moving from one state to another between timesteps.
å‡è®¾ï¼šstationary distribution
## mini-forward algorithm
$$
P(W_{i+1}) = \sum_{w_i} P(w_i, W_{i+1}) \quad \text{chain rule} \quad \Rightarrow \quad P(W_{i+1}) = \sum_{w_i} P(W_{i+1} \mid w_i) P(w_i)

$$
å·²çŸ¥transition modelï¼š$P(W_{i+1}|W_i)$å°±å¯ä»¥æ±‚è§£$P(W_{i+1})$
## Stationary Distribution
å½“ç»è¿‡ä¸€å®šé‡æ—¶é—´ä¹‹å$P(W_{i+1})=P(W_{i})$
![[image-8.png]]
è§£ä¸Šè¿°çš„æ–¹ç¨‹ã€‚
Â In general, ifÂ Â $W_t$had a domain of size k, the equivalence $P(W_{i+1}) =  \sum_{w_i} P(W_{i+1} \mid w_i) P(w_i)$Â Â yields a system of k equations, which we can use to solve for the stationary distribution.

# Hidden Marko Models
HMMï¼šallows us to observe some evidence at each timestep, which can potentially affect the belief distribution at each of the states. Compared to the Markov model, the Hidden Markov model requires not only the initial distribution, the transition model, but also theÂ **sensor model.**![[image-17.png|Wi:state variable Fi an evidence variable]]
![[image-24.png]]
æ€§è´¨ï¼š
1. ![[image-18.png]]
2. ![[image-19.png]]
3. ![[image-20.png]]
- Wiï¼šéšè—çŠ¶æ€ï¼ˆæ¯”å¦‚ç³»ç»Ÿçš„çœŸå®çŠ¶æ€ï¼‰
- Fiï¼šè§‚æµ‹å€¼ï¼ˆä½ èƒ½çœ‹åˆ°çš„ä¿¡å·ï¼‰
- æ¯ä¸ªçŠ¶æ€åªä¾èµ–äºå‰ä¸€ä¸ªçŠ¶æ€ï¼ˆé©¬å°”å¯å¤«æ€§ï¼‰
- æ¯ä¸ªè§‚æµ‹å€¼åªä¾èµ–äºå½“å‰çŠ¶æ€ï¼ˆå±€éƒ¨æ€§ï¼‰
![[image-21.png|605x185]]
![[image-25.png]]
![[image-26.png]]
è§£é‡Šï¼šå½“æˆ‘ä»¬è·å–åˆ°è§‚æµ‹ä¹‹åï¼Œä¸ç¡®å®šæ€§ä¼šé™ä½ï¼
![[image-29.png|æœ‰äº†å¸¦ä¼çš„æ¡ä»¶ä¹‹åRainçš„æ¦‚ç‡ä¸Šå‡ï¼Œç¬¬äºŒå¤©æ¦‚ç‡å‡å°‘æ˜¯å› ä¸ºä¸ç¡®å®šæ€§å¢åŠ ï¼Œä½†å¦‚æœè§‚æµ‹åˆ°ä¸¤æ¬¡å¸¦ä¼ï¼ŒRainçš„æ¦‚ç‡åˆä¼šä¸Šå»]]


## Viter Algorithm
![[image-30.png]]
edge weightï¼š![[image-31.png]]
![[image-32.png]]

![[image-33.png|ä¸å‰é¡¹å…³ç³»]]
ç¬¬ä¸€éä»å‰åˆ°åå¾ªç¯ï¼šè®°å½•èƒ½è®©mæœ€å¤§çš„xï¼Œç¬¬äºŒæ¬¡ä»åå‘å‰å¾ªç¯ï¼Œèµ°å·²ç»æ‰¾åˆ°çš„æœ€ä¼˜è·¯çº¿![[image-34.png]]

HMMç¼ºç‚¹ï¼šHidden Markov Models have the same drawback as bayes net - the time it takes to run exact inference with the forward algorithm **scales with the number of values in the domains of the random variables**. 
## Particle Filtering
ç±»ä¼¼äºè´å¶æ–¯å‡€é‡‡æ ·çš„éšé©¬å°”å¯å¤«æ¨¡å‹ç§°ä¸º**ç²’å­æ»¤æ³¢**ï¼Œæ¶‰åŠé€šè¿‡çŠ¶æ€å›¾æ¨¡æ‹Ÿä¸€ç»„ç²’å­çš„è¿åŠ¨ï¼Œä»¥è¿‘ä¼¼ç›¸å…³éšæœºå˜é‡çš„æ¦‚ç‡ï¼ˆä¿¡å¿µï¼‰åˆ†å¸ƒã€‚æˆ‘ä»¬è®¤ä¸ºç²’å­åœ¨ä»»ä½•ç»™å®šæ—¶é—´æ­¥é•¿å¤„äºä»»ä½•ç»™å®šçŠ¶æ€çš„ä¿¡å¿µå®Œå…¨å–å†³äºæˆ‘ä»¬æ¨¡æ‹Ÿä¸­è¯¥æ—¶é—´æ­¥é•¿å¤„äºè¯¥çŠ¶æ€çš„ç²’å­æ•°é‡ã€‚è¦ä»ç²’å­åˆ—è¡¨ä¸­æ¢å¤ä¿¡å¿µåˆ†å¸ƒï¼Œæ‚¨éœ€è¦åšçš„å°±æ˜¯è®¡ç®—ç²’å­çš„æ•°é‡å¹¶å¯¹å…¶è¿›è¡Œå½’ä¸€åŒ–ã€‚
- Simulationï¼š
- ### 1ï¸âƒ£ ç²’å­åˆå§‹åŒ–ï¼ˆParticle Initializationï¼‰

- éšæœºç”Ÿæˆä¸€ç»„ç²’å­ï¼Œæ¯ä¸ªç²’å­ä»£è¡¨ä¸€ä¸ªå¯èƒ½çš„åˆå§‹çŠ¶æ€ã€‚
    
- é€šå¸¸æ˜¯ä»å…ˆéªŒåˆ†å¸ƒä¸­é‡‡æ ·ï¼Œæ¯”å¦‚å‡åŒ€åˆ†å¸ƒæˆ–é«˜æ–¯åˆ†å¸ƒã€‚
    

### 2ï¸âƒ£ æ—¶é—´æ¨è¿›æ›´æ–°ï¼ˆTime Elapse Updateï¼‰

- æ ¹æ®**çŠ¶æ€è½¬ç§»æ¨¡å‹**ï¼ˆtransition modelï¼‰æ›´æ–°æ¯ä¸ªç²’å­çš„çŠ¶æ€ã€‚
    
- æ¨¡æ‹Ÿç³»ç»Ÿéšæ—¶é—´æ¼”åŒ–çš„è¿‡ç¨‹ã€‚
    

ä¾‹å¦‚ï¼šå¦‚æœç²’å­è¡¨ç¤ºä¸€ä¸ªäººçš„ä½ç½®ï¼ŒçŠ¶æ€è½¬ç§»æ¨¡å‹å¯èƒ½æ˜¯â€œæ¯ç§’ç§»åŠ¨ Â±1 ç±³â€ã€‚

### 3ï¸âƒ£ è§‚æµ‹æ›´æ–°ï¼ˆObservation Updateï¼‰

- æ ¹æ®**è§‚æµ‹æ¨¡å‹**ï¼ˆsensor modelï¼‰è¯„ä¼°æ¯ä¸ªç²’å­çš„â€œå¯ä¿¡åº¦â€ã€‚
    
- ç»™æ¯ä¸ªç²’å­åˆ†é…ä¸€ä¸ªæƒé‡ï¼Œè¡¨ç¤ºå®ƒä¸å½“å‰è§‚æµ‹å€¼çš„åŒ¹é…ç¨‹åº¦ã€‚![[image-35.png]]
    ![[image-36.png]]
- ç„¶åæ ¹æ®è¿™äº›æƒé‡è¿›è¡Œ**é‡é‡‡æ ·**ï¼ˆresamplingï¼‰ï¼Œä¿ç•™é«˜æƒé‡ç²’å­ï¼Œä¸¢å¼ƒä½æƒé‡ç²’å­ã€‚

## Utilitiesï¼š
![[image-37.png]]
- **Axioms of Rationality**ï¼š
- ![[image-38.png]]
- ![[image-39.png]]
## Decision Network
In decision network: 
- Chance node (ovals) - Chance nodes in a decision network behave identically to Bayesâ€™ nets. 
- Action node (rectangles) - Action nodes are nodes that we have complete control over. 
- Utility node (diamonds) - Utility nodes are children of some combination of action and chance nodes.
- ![[image-40.png]]
**GOALï¼š**Â select the action which yields theÂ **maximum expected utility**(MEU), and the expected utility of taking an action a given evidence e and n chance nodes is computed with
![[image-41.png|aæ˜¯action]]
![[image-42.png|bä»£è¡¨é›¨ä¼ï¼ŒWæ˜¯éšæœºçš„å¤©æ°”]]
### The Value of Perfect Information (VPI)
Â mathematically quantifies the amount an agentâ€™s maximum expected utility is expected to increase if it observes some new evidence.
Â ![[image-43.png|sæ˜¯set of states]]
Â ç”±äºå®é™…ä¸Šå¹¶ä¸çŸ¥é“æ–°çš„evidenceæ˜¯å•¥ï¼Œæ‰€ä»¥ç”¨æ¦‚ç‡åˆ†å¸ƒ![[image-44.png]]
Â ä¸€äº›æ€§è´¨ï¼š![[image-45.png]]




## Markov Decision Processes
å®šä¹‰çš„propertiesï¼š
![[image-46.png]]
- **markovianess**
    - if we know the present state, knowing the past doesnâ€™t give us any more information about the future
    - $$ P(St+1â€‹=st+1â€‹âˆ£Stâ€‹=stâ€‹,Atâ€‹=atâ€‹,...,S0â€‹=s0â€‹)=P(St+1â€‹=st+1â€‹âˆ£Stâ€‹=stâ€‹,Atâ€‹=atâ€‹)$$
    - $$ T(s,a,sâ€²)=P(sâ€²âˆ£s,a)$$
### Finite Horizons and Discount factors
è®¾ç½®ä¸€äº›æ—¶é—´é™åˆ¶
![[image-47.png|é‡‡ç”¨ç‰¹å®šçš„gammaè®©Uæœ‰ä¸€ä¸ªä¸Šé™\æŠ˜æ‰£å› å­ Î³âˆˆ[0,1]ï¼šæœªæ¥å¥–åŠ±çš„è¡°å‡ç¨‹åº¦]]
### Solving MDP with the Bellman Equation
MDP(makov decision process)ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ª**æœ€ä¼˜ç­–ç•¥** Ï€âˆ—:Sâ†’A\pi^*: S \to Aï¼Œä½¿å¾—åœ¨æ¯ä¸ªçŠ¶æ€ä¸‹é€‰æ‹©çš„åŠ¨ä½œèƒ½æœ€å¤§åŒ–é•¿æœŸç´¯è®¡å¥–åŠ±ã€‚
ğŸ” çŠ¶æ€å€¼å‡½æ•° $U^*(s)$
è¿™æ˜¯æ¯ä¸ªçŠ¶æ€çš„æœ€ä¼˜ä»·å€¼ï¼Œå®šä¹‰ä¸ºï¼š
$U^*(s) = \max_a Q^*(s, a)$
æ„æ€æ˜¯ï¼šåœ¨çŠ¶æ€ s ä¸‹ï¼Œé€‰æ‹©æœ€ä¼˜åŠ¨ä½œ a æ‰€èƒ½è·å¾—çš„æœ€å¤§æœŸæœ›å›æŠ¥ã€‚

ğŸ” Qå€¼å‡½æ•°$Q^*(s, a)$
è¿™æ˜¯åœ¨çŠ¶æ€ ss æ‰§è¡ŒåŠ¨ä½œ aa çš„æœŸæœ›å›æŠ¥ï¼Œå®šä¹‰ä¸ºï¼š$Q^*(s, a) = \sum_{s'} T(s, a, s') \left[ R(s, a, s') + \gamma U^*(s') \right]$
è§£é‡Šå¦‚ä¸‹ï¼š
- å¯¹æ‰€æœ‰å¯èƒ½çš„ä¸‹ä¸€çŠ¶æ€ sâ€² æ±‚å’Œ
- æ¯ä¸ªé¡¹æ˜¯ï¼šè½¬ç§»æ¦‚ç‡ Ã—ï¼ˆå³æ—¶å¥–åŠ± + æŠ˜æ‰£åçš„æœªæ¥ä»·å€¼ï¼‰
è¿™å°±æ˜¯**Bellmanæ–¹ç¨‹çš„Qå½¢å¼**ï¼Œå®ƒä½“ç°äº†**æœŸæœ›æœ€å¤§åŒ–åŸåˆ™**ã€‚

ğŸŒ³ æœç´¢æ ‘ä¸ä¸ç¡®å®šæ€§
ä½ æåˆ°â€œstate-space graphs can be unraveled into search treesâ€ï¼Œè¿™æ˜¯è¯´ï¼š
- MDPå¯ä»¥å±•å¼€æˆä¸€ä¸ªæœç´¢æ ‘ï¼Œæ¯ä¸ªèŠ‚ç‚¹æ˜¯çŠ¶æ€ï¼Œæ¯ä¸ªåˆ†æ”¯æ˜¯åŠ¨ä½œ
- åœ¨è¿™ç§æ ‘ä¸­ï¼Œä¸ç¡®å®šæ€§é€šè¿‡**Q-stateèŠ‚ç‚¹**å»ºæ¨¡ï¼Œå®ƒä»¬ç±»ä¼¼äº**expectimaxä¸­çš„chanceèŠ‚ç‚¹**
ä¹Ÿå°±æ˜¯è¯´ï¼ŒQå€¼å‡½æ•°åœ¨æœç´¢æ ‘ä¸­æ‰®æ¼”äº†â€œæœŸæœ›è¯„ä¼°â€çš„è§’è‰²ï¼Œå¸®åŠ©æˆ‘ä»¬åœ¨ä¸ç¡®å®šæ€§ä¸‹åšå‡ºæœ€ä¼˜å†³ç­–