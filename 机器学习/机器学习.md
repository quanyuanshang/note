机器学习

some notation: 

1. training set: get the model from it

   x=input,y=output,m=number of training examples
   $$
   (x^{i},y^{i})上标i代表第i个训练示例
   $$
   ![image-20250118121909461](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250118121909461.png)
































































**supervised learning**

learn from given right answers

- classification

  predict category

  single input 

  multiple input

![image-20250118114744427](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250118114744427.png)

find the boundary line

**Unsupervised learning**

DATA only comes with inputs x,but not out put labels y.

Algorithm has to find the structure in the data.

- clustering:

  获取没有标签的数据并且尝试自动将它们分组到集群中

- anomaly detection

- dimensionality reduction

  # Linear regression

  predict a number (infinitely many possible outputs

  - linear regression

  - cost function(for only one feature)
    $$
    J(w,b)=\frac{1}{2m}\sum_{i=0}^m(\hat{y}^{(i)}-y^{(i)})^2
    $$

    $$
    J(w,b)=\frac{1}{2m}\sum_{i=0}^m(f_{w,b}(x^{(i)})-y^{(i)})^2
    $$

    最小化J(w,b)来找到最佳参数w,b.

    持续更新w，b直到最终结果收敛达到局部最小值（实际上就是找到极值点，即导数为0）

    两者的更新是同时的
    $$
    w=w-\alpha \frac{\delta J(w,b)}{\delta w}\\
    =w-\frac{1}{m}\sum_{i=0}^m(f_{w,b}(x^{(i)})-y^{(i)})x^{(i)}\\
    b=b-\alpha \frac{\delta J(w,b)}{\delta b}\\
    =b-\frac{1}{m}\sum_{i=0}^m(f_{w,b}(x^{(i)})-y^{(i)})\\
    $$

  alpha 是学习率（0到1之间）。控制更新模型参数w和b时采取的步骤大小。过小就太慢了，过大就容易跳过最小值 

  这叫做batch gradient descent：在计算grade descent的时候都要使用到所有的训练样例

  - ## 梯度下降

    Outline:

    1. start with some a,b
    2. keep changing w,b to reduce J(w,b)
    3. until near min   

  ![image-20250120220303786](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250120220303786.png)

代码：

计算代价函数在固定w与b时的值。

```python
def computeCost(X, y, theta):
    inner = np.power(((X * theta.T) - y), 2)
    return np.sum(inner) / (2 * len(X))

data.insert(0, 'Ones', 1)#再多加一列是用来计算常数b

# set X (training data) and y (target variable)
cols = data.shape[1]
X = data.iloc[:,0:cols-1]#X是所有行，去掉最后一列
y = data.iloc[:,cols-1:cols]#y是所有行的最后一列

#将XY转化为numpy矩阵进行计算
X = np.matrix(X.values)
y = np.matrix(y.values)
theta =np.matrix(np.array([0,0]))#这里theta取二维是因为要与X的维度保持一致
computeCost(X, y, theta)

def gradientDescent(X, y, theta, alpha, iters):
    temp = np.matrix(np.zeros(theta.shape))
    parameters = int(theta.ravel().shape[1])#要求的是X有多少个特征值
    cost = np.zeros(iters)
    
    for i in range(iters):
        error = (X * theta.T) - y#这是数组的算法最后得到的还是个数组
        
        for j in range(parameters):#第j项特征值(实际上就2项 j=0,1
            term = np.multiply(error, X[:,j])#两个数组形状相同的进行每一项对应相乘形成一个新的数组对于求常数而言乘的X【：，0】=1
            temp[0,j] = theta[0,j] - ((alpha / len(X)) * np.sum(term))
            
        theta = temp
        cost[i] = computeCost(X, y, theta)
        
    return theta, cost

```

多元线性回归的梯度下降算法：

**vectorization：**

w=[b,w1,...,wn]

x=[1,x1,...,xn]

对于多个特征：

x=\[[1,x_1^1,x_2^1,...x_n^1],

[1,x_1^2,...],

...,

[1,x_1^n,...]]

![image-20250220172054901](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250220172054901.png)

![image-20250223163703836](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250223163703836.png)

![image-20250223163723338](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250223163723338.png)

![image-20250126200934641](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250126200934641.png)

![image-20250126201057105](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250126201057105.png)

![image-20250126201911172](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250126201911172.png)



现在把cost function写成(for n features)
$$
J(\vec{w},b)
$$
![image-20250126203154115](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250126203154115.png)

>  如何选择w_1,w_2

当x_1比较大，当w_1出现非常小的变换都会对成本函数产生比较大的变化，所以w_1会比较小。

此时的contours就会变成窄窄的椭圆，之后会反复弹跳选择合适的w。

目标是把它转化为一个标准的圆，也就是把x_1和x_2数量范围尽量统一

![image-20250126204612658](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250126204612658.png)

- feature scaling:

  目标是让每个x_j的范围尽量接近（不要有数量级差距）

  - mean normalization

  - $$
    x_1\in[a,b]\\
      x_1=\frac{x_1-\mu_1}{b-a}
    $$

  - 直接除以b

    ![image-20250126205529863](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250126205529863.png)

  - z-score

    ![image-20250126205239978](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250126205239978.png)

  

- 判断梯度下降是否收敛

  ![image-20250220164412795](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250220164412795.png)

- 如何选择学习率：

倾向于选择小的alpha，能确保成本函数递增，但是太小的alpha最终会使成本函数收敛时间会增长。

![image-20250220164751255](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250220164751255.png)



- feature engineering:调整原有的feature或形成新的更好的feature

  比如有时候只有x^2,那么y最终会下降，但是预期是上升，所以采用![image-20250220170132898](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250220170132898.png)会更合理，，并且对于数量级不同的x^2和x^3，可以用feature scaling来进行优化。





假设点到直线距离符合正态分布：

![image-20250417104757573](assets/image-20250417104757573.png)

```
loss = len(distances)/2 * np.log(2*np.pi*sigma**2) + np.sum(distances**2)/(2*sigma**2)
```

# Logistics Regression

sigmod function：

![image-20250220212151089](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250220212151089.png)
$$
regression \ mode：
f_{\vec{w},b}(\vec{x})=g(\vec{w}.\vec{x}+b)=\frac{1}{1+e^{-(\vec{w}\vec{x}+b)}}\\
f_{\vec{w},b}(\vec{x})=P(y=1|\vec{x};\vec{w},b)
$$
将模型的结果看成y=0或1的概率

- **decision boundary:**

​	z=wx+b=0

假设让P>0.5,y=1，那么z大于0时，y=1,而z=0 is decision boundary

- cost function
  $$
  J(\vec{w},b)=\frac{1}{m}\sum_{i=1}^{m}L(f_{\vec{w},b}(\vec{x}^{i},y^{i}))
  $$
  ![image-20250220214025355](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250220214025355.png)

  ![image-20250220215218865](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250220215218865.png)

  ![image-20250220215328412](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250220215328412.png)

当y事实上是1，而f预测出来概率只有0.1，此时的loss会很大。但是当f预测出来y有100%的概率会是1，那么loss就是0.

这样的cost function就是convex，从而能到达全局最小

- 如何找到更好的参数w，b，形式看上去和线性回归很像，但是预测函数不同。

![image-20250220220157064](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250220220157064.png)

feature scaling also works!



- overfitting =high variance

- underfitting=high bias

  很容易受噪声影响：

  ![image-20250222233526791](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250222233526791.png)

  ![image-20250222233826907](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250222233826907.png)

  

  解决方法：

  1. collect more data

  2. use fewer feature（采用最有用的feature）

     原本f(x)=x_1+x_2^2+x_3^3+...

     改为f(x)=x_1+x_3^2就可能会

  3. **regularization**

     保留所有的feature并避免有feature产生过大的影响。即使涉及到了高阶多项式，也可以通过减小各项之前的参数来减弱overfit。

      new cost function
     $$
     J(\vec{w},b)=min(\frac{1}{2m}\sum_{i=1}^m (f_{\vec{w},b}(\vec{x^{(i)}})-y^{(i)})^2+\frac{\lambda}{2m}\sum_{j=1}^n w_j^2)
     $$
     \lambda 的选择能balance 两部分（尽可能让偏差变小的同时，不会出现overfit，也就是让各个系数都尽可能小），但如果说x都取得特别特别小，那么就会造成欠拟合。

     - regularized linear regression

        ![image-20250222235717126](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250222235717126.png)

     - regularized logistic regression

       ![image-20250223161738178](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250223161738178.png)

​		

向量化：

维度：m个样本，n个特征

![image-20250223163833119](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250223163833119.png)

Xtheta是各个特征乘上各自权重的线性组合：

*号指的是各个位置对应相乘

![image-20250223163112524](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250223163112524.png)

![image-20250223163808728](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250223163808728.png)

![image-20250223171739695](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250223171739695.png)

正则化分情况对theta做：

![image-20250223181804755](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250223181804755.png)



multiple classification:



![image-20250308193136466](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250308193136466.png)

logistic perception 的优势：还能描述样本出现的不确定性（P(y|theta)).perception 只会在分类错误的时候进行更新。但是Logistics分类的时候，是即便分类正确，但是为了让MLE更大，让loss更小，还是会更新参数。

# Neural network

神经网络neural network：

通过学习自己的特征来形成更好的特征进行逻辑回归，从而做出更准确的判断。![image-20250224193736081](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250224193736081.png)

参数的上标代表着在第几层。每一层的输出是下一层的输入

![image-20250302224822160](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250302224822160.png)

![image-20250302225328576](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250302225328576.png)

activation function：g()可以是sigmoid![image-20250302225730111](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250302225730111.png)

以上都是forward propagation(向前传递)



- build a neural network

  1. 

  ```py
  x=np.array([[200.0,17.0]])
  layer_1=Dense(units=3,activation="sigmoid")
  a1=layer_1(x)
  ```

  2. sequential

     ```py
     x=np.array([[200.0,17.0]])
     y=np.array([1])
     layer_1=Dense(units=3,activation="sigmoid")
     layer_2=Dense(units=1,activation="sigmoid")
     model=Sequential([layer_1,layer_2])
     model.compile(...)
     model.fit(x,y)
     model.predict(x_new)
     ```

     .fit(),let tensflow take the neural network stringed together by diff layers and train it on data X and Y.And we can omit the procedure a1 a2 ...

     more commonly write:

     ```py
     model=Sequential([
     Dense(units=3,activation="sigmoid"),
     Dense(units=1,activation="sigmoid")
     ])
     layer_1=Dense(units=3,activation="sigmoid")
     layer_2=Dense(units=1,activation="sigmoid")
     model.compile(...)
     model.fit(x,y)
     model.predict(x_new)
     ```

  3. 具体的实现：

     W[:,j]取出第j列的每一行



激活函数的不同选择，如果想让a更大的话：rectified linear function：（ReLu）g（z）=max（0，z）

如何选择？

1. 在输出层：

二元分类：sigmoid y=0/1

regression：linear function y=+-

ReLu: y>=0

![image-20250401113850769](assets/image-20250401113850769.png)

2. 在hidden layer：

   建议使用ReLu function：

   原因：平坦部分只有一半，而不是像sigmoid有两部分，做梯度下降会优化一点，同时计算比sigmoid简单。![image-20250401114250822](assets/image-20250401114250822.png)

   

## 多分类问题

![image-20250401114850131](assets/image-20250401114850131.png)

### Softmax：

如何计算output：

![image-20250401115436864](assets/image-20250401115436864.png)

![image-20250401115337391](assets/image-20250401115337391.png)

loss function：

![image-20250401120102681](assets/image-20250401120102681.png)

TensorFlow：

![image-20250401120141019](assets/image-20250401120141019.png)

改进的代码版本：解决四舍五入的误差问题：

![image-20250401125848123](assets/image-20250401125848123.png)

## Multi-label Classification

![image-20250417111844703](assets/image-20250417111844703.png)

和多分类问题区分开来。这里是相当于把一张图片上同时检测是否还有car、bus和人行道。

## Optimizer：

Adam：（自动优化learning rate，会比梯度下降更快）

![image-20250417111950864](assets/image-20250417111950864.png)

## Additional Neural Network Concept：

CNN：

Each neuron only looks at part of the previous layer's inputs.

![image-20250417112609448](assets/image-20250417112609448.png)



# Evaluate Model

Split the train set into three part:

![image-20250306110633165](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250306110633165.png)

 

![image-20250306110759147](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250306110759147.png)

in different model, choose para based on train set , and select the model with the min J_cv, and estimate generalization error using test set.

![image-20250306111150753](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250306111150753.png)

J_cv in classification can simply be defined as the error rate.



Bias/Variance

![image-20250314140500218](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250314140500218.png)

拟合的多项式维度越高，越能fit训练集。

也有可能会部分overfit trainset，部分underfit trainset最终同时出现高偏差和高方差

![image-20250314140917391](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250314140917391.png)

如何选择正则项的lambda：

1. large lambda:

   w will be very small in this case and the predict y will be b approximately.

2. small lambda:

   Similarly no regularization, J_cv will be larger than J_train.

   ![image-20250314142306122](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250314142306122.png)

   different lambda will result in different w,b

   ![image-20250314142959726](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250314142959726.png)

   ![image-20250314145039180](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250314145039180.png)有的时候样本就是有很多噪音，无法期待error很小，那么就需要baseline作为评判参考。





High bias:

If a learning algorithm suffers from high bias, getting more training data will not help much. 

High variance:

getting more data will help.



Debugging a learning algorithm:

![image-20250314151518091](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250314151518091.png)



Neural networks and bias variance:



![image-20250314152127829](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250314152127829.png)









With  proper regularization， a larger neural network never hurt(with more hidden layer)

# Decision Tree

- discrete

one hot encoding:

![image-20250226103301431](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250226103301431.png)

![image-20250226103447863](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250226103447863.png)

- continuous feature:

  拆分时，只需要考虑different value to split on,carry out the usual information gain caculation and decide to split on the one giving the highest information gain



- tree ensemble

  **Problem:**

  ​	Once change one train example,will build a totally different tree.

  ​	Such algorithm is not robust. But with a bunch of decision trees,will be more accurate. Once get a new test example, let the trees vote.

  ​	

  **Solution:**

  - **sampling with replacement:** construct a new training set.

  - Why: explore small changes of the data and average it,so that little change further to the training set make it less likely to have a huge impact on the overall  output of the overall random forest algorithm.

  - RNDOM FOREST：

  how to generate a tree ensemble:

  B：#number of decision trees, when B beyond a certain point, you end up with diminishing returns.

  ![image-20250226105510665](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250226105510665.png)

   

  - Randomizing the feature choice:

  for lager n:

  ![image-20250226110032180](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250226110032180.png)

  ​	k tipically:\sqart{n}

​	

- XGBoost:

  ![image-20250226111517573](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250226111517573.png)

  ![image-20250226111618563](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250226111618563.png)

  ![image-20250226111648098](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20250226111648098.png)


# K-means

Clustering：自动找相似的数据

 ![image-20250417113212878](assets/image-20250417113212878.png)



K-means：

- 随机猜测cluster的中心在哪里【cluster centroids】

  ![image-20250417113647338](assets/image-20250417113647338.png)

1. 将点分配给cluster centroids

   ![image-20250417113824353](assets/image-20250417113824353.png)

   遍历样本中的每一个点，查看它是否更接近与红的centroid还是蓝的centroid

2. 移动 cluster centroids

   计算所有红色点的平均位置设置为新的centroids。蓝色同样。

   再次重复步骤一，检查所有样本点是否更接近于centroids
   
   ![image-20250417130909764](assets/image-20250417130909764-1744866600222-1.png)

当最后centroids的位置不再变化。会收敛。

![image-20250417132036486](assets/image-20250417132036486.png)

计算情况，如果说有一个聚类里面没有任何样本，那么不要这个聚类了，K-=1.

cost/distortion function：

![image-20250417132632957](assets/image-20250417132632957.png)

每个样本点到聚类距离平方之和的最小值。

为什么求均值能最小化cost function？

基本不等式

![image-20250417133223660](assets/image-20250417133223660.png)

- initialize K-means：

  random initialization

  Randomly pick K training examples.多次随机开始选择并选取其中计算出的J最小的那个分类。

  ![image-20250417134043730](assets/image-20250417134043730.png)

  ![image-20250506133651324](assets/image-20250506133651324.png)先随机选择一个点再取其他相距最远的点，但是可能会受到outlier的影响。

  ![image-20250506133818063](assets/image-20250506133818063.png)

  第一个点也是随机选择，周围的点被选中成为中心的概率与第一个被选到的距离成正比，可以处理outlier。

- choose the number of clusters:

  自动选择的一些方法：

  elbow method（不是很好）

  手动选择需要权衡精细度和成本

# Anomaly Detection

需要fit的model是概率模型

![image-20250422103636336](assets/image-20250422103636336.png)

![image-20250426165300632](assets/image-20250426165300632.png) 



real-number evaluation：

monitor example：

将异常sample分到validation set和test set上能帮助更好的调整epsilon的大小，如果异常样本量太小，就只放到validation上（但没有办法来验证这个算法到底做的怎么样）

既然有了标签（知道是不是异常事件）为什么不直接用监督学习的方法？

异常事件检测：当异常事件数量少，并且当未来的异常可能与目前得到的异常事件不一样。相当于从正常事件里学习eg：花样层出不穷的诈骗

监督学习：相当于从异常事件来学习，假设未来遇到的异常事件会和学习的异常事件相似eg：模式固定的spam email

![image-20250426172412914](assets/image-20250426172412914.png)



特征选择：

让特征分布变成高斯从而更好拟合。 

如果有异常事件没有被检测到，可以尝试加入其他feature

![image-20250426173602775](assets/image-20250426173602775.png)



# Recommand system

![image-20250428173334560](assets/image-20250428173334560.png)

目标是预测用户会如何评价他们没看过的电影。

算法设计：

![image-20250428173925450](assets/image-20250428173925450.png)预测人物j对电影i的评分，引入电影特征向量x：
$$
j's\ rating: w^j.x^i+b^j
$$
![image-20250428174416185](assets/image-20250428174416185.png)

仅对用户评分过的电影进行计算损失。

![image-20250428174600813](assets/image-20250428174600813.png)

如果没有feature描述怎么办：
Collaborative filtering:

从数据中learn feature，即是一部怎样的电影让用户打出了这个评分体系。之所以能够猜测feature是因为存在不同的用户，但在前面的linear regression中，是没办法仅靠一个用户就提取数据特征的。

根据已经知道的用户评分和假设的w来取某个x最小化cost function：

![image-20250428175509763](assets/image-20250428175509763.png)

再结合上面的cost function：

![image-20250428175730100](assets/image-20250428175730100.png)

Gradient Descent：

![image-20250428175935377](assets/image-20250428175935377.png)



binary label:喜欢还是不喜欢

![image-20250428211950182](assets/image-20250428211950182.png)

loss：![image-20250428212129051](assets/image-20250428212129051.png)

均值归一化:mean normalization

 对于没有对任何电影进行评分的新用户，初始化w和b都是0的话，那么预测他的分数都是0，这不是预期的结果。

对每个电影计算平均分数。

![image-20250428212806220](assets/image-20250428212806220.png)

这样就把猜测的分数设置为了均值

finding related items

 找到特征值相差最小的

![image-20250428213629459](assets/image-20250428213629459.png)

limitations：

- 冷启动

对于新项目

对于新用户

- 使用了side information





## Content based filtering algorithm

![image-20250506185805789](assets/image-20250506185805789.png)

content...:用户和物品都有特征向量。

![image-20250506190003473](assets/image-20250506190003473-1746529204770-1.png)

两个向量可能数据大小很不一样。

![image-20250506190536935](assets/image-20250506190536935.png)

但是做点积的时候两个向量大小要一样。所以可以用神经网络来提取一定维度的特征。（神经网络降低维度）

![image-20250506190903354](assets/image-20250506190903354.png)



大型目录中的推荐

1. retrieval

   ![image-20250506191842573](assets/image-20250506191842573.png)

2. ranking

   ![image-20250506192043056](assets/image-20250506192043056.png)







# PCA

主要成分分析

减少特征数，帮助可视化数据。

原理：因为有的时候一个特征变化明显，但是另一个特征相比起来就变化甚微这个时候会考虑去掉变化不明显的特征。![image-20250506192423889](assets/image-20250506192423889.png)

有的时候几个特征都很重要就会去找新的坐标轴和坐标俩整合多个特征

![image-20250506192707330](assets/image-20250506192707330.png)

![image-20250506192811956](assets/image-20250506192811956.png)



方法：

![image-20250506193349811](assets/image-20250506193349811.png)

选择一个轴取映射，这个轴信息squished，挤在一起，信息量就缺少了。

![要选择能让分布最大化](assets/image-20250506193559305-1746531359998-3.png)

用点积计算映射到轴上的距离

每次选取的轴都和前面所有的轴垂直

![image-20250506194048906](assets/image-20250506194048906.png)

reconstruction：

根据映射长度回推坐标。

![image-20250506213427204](assets/image-20250506213427204.png)

所谓的PCA就是将原数据投影到原空间的一个子空间，并使得其保留的信息尽可能多

应用：减少特征数量让监督学习算法更快。



![image-20250520132816349](assets/image-20250520132816349-1747718896805-1.png)

协方差：不同特征之间的关系。

![image-20250520133339374](assets/image-20250520133339374.png)

优化的函数：

第一步先中心化x，这样能让v从原来开始，简化计算。

![image-20250520134856972](assets/image-20250520134856972.png)

minimize reconstruction error=maximize x 在v方向上投影的方差。

![image-20250520134016004](assets/image-20250520134016004.png)

![image-20250520134408570](assets/image-20250520134408570.png)

接下来再加上一个限制条件确保v是单位向量。![image-20250520140223840](assets/image-20250520140223840.png)

于是能得到lambda是XTX特征值，v是特征向量

![image-20250520140319305](assets/image-20250520140319305.png)

![image-20250520141547938](assets/image-20250520141547938.png)

这个公式代表的是方差贡献率。variance总共取的更少，丢掉的不重要的东西越多，但在小的variance下还留存的特征一定是比较重要的。

具体过程：![image-20250520210133479](assets/image-20250520210133479.png) 
